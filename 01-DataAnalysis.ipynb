{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Annotation Dataset\n",
    "Script to convert text + label into ner format supported by GliNER or Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63862</th>\n",
       "      <td>ID_909433914506215424</td>\n",
       "      <td>RT @FLAFLCIO: Visit our new website to apply f...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6305</th>\n",
       "      <td>ID_1032287088190808064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70452</th>\n",
       "      <td>ID_913625501440380928</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25148</th>\n",
       "      <td>ID_1109454892576452608</td>\n",
       "      <td>As the sun sets, a 2nd truck is loaded and rea...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56714</th>\n",
       "      <td>ID_906167049931632640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72742</th>\n",
       "      <td>ID_914857829684695040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Puerto Rico U.S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>ID_1032637816096808960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kerala</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37160</th>\n",
       "      <td>ID_1176567269553557504</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27470</th>\n",
       "      <td>ID_1112343800700121088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5711</th>\n",
       "      <td>ID_1032133583165513728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_id  \\\n",
       "63862   ID_909433914506215424   \n",
       "6305   ID_1032287088190808064   \n",
       "70452   ID_913625501440380928   \n",
       "25148  ID_1109454892576452608   \n",
       "56714   ID_906167049931632640   \n",
       "72742   ID_914857829684695040   \n",
       "7019   ID_1032637816096808960   \n",
       "37160  ID_1176567269553557504   \n",
       "27470  ID_1112343800700121088   \n",
       "5711   ID_1032133583165513728   \n",
       "\n",
       "                                                    text          location  \n",
       "63862  RT @FLAFLCIO: Visit our new website to apply f...               NaN  \n",
       "6305                                                 NaN            Kerala  \n",
       "70452                                                NaN     United States  \n",
       "25148  As the sun sets, a 2nd truck is loaded and rea...               NaN  \n",
       "56714                                                NaN           Florida  \n",
       "72742                                                NaN  Puerto Rico U.S.  \n",
       "7019                                                 NaN            Kerala  \n",
       "37160                                                NaN          Pakistan  \n",
       "27470                                                NaN               NaN  \n",
       "5711                                                 NaN               NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLDER = \"microsoft-learn-location-mention-recognition-challenge20240905-10153-193u9hv\" # update this path https://zindi.africa/competitions/microsoft-learn-location-mention-recognition-challenge/data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs(\"data/accepted_data\", exist_ok=True)\n",
    "\n",
    "train = pd.read_csv(os.path.join(FOLDER, \"Train_1.csv\"))\n",
    "\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73072"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id        0\n",
       "text        56624\n",
       "location    29612\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48059"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.dropna(subset=[\"text\", \"location\"], how=\"all\")\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id        0\n",
       "text        31611\n",
       "location     4599\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text can't be Nan so dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16448"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.dropna(subset=[\"text\"], how=\"all\")\n",
    "\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17903</th>\n",
       "      <td>ID_1062043965636132864</td>\n",
       "      <td>As three record-breaking wild fires continue t...</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37119</th>\n",
       "      <td>ID_1176558963938254848</td>\n",
       "      <td>4/4 Doctors and paramedics are providing neces...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43773</th>\n",
       "      <td>ID_784649154194710528</td>\n",
       "      <td>Flash Flood Warning for Bamberg, Calhoun, Clar...</td>\n",
       "      <td>Bamberg Calhoun Clarendon Orangeburg Richland ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71527</th>\n",
       "      <td>ID_914176142327914496</td>\n",
       "      <td>Now this is something I can trust. DONATE!!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14981</th>\n",
       "      <td>ID_1041985289210785792</td>\n",
       "      <td>Death toll so far from #HurricaneFlorence 34Tr...</td>\n",
       "      <td>Puerto Rico</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69724</th>\n",
       "      <td>ID_913251069802692608</td>\n",
       "      <td>Hurricane Maria : Charity Navigator list of ch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40724</th>\n",
       "      <td>ID_732607952192835584</td>\n",
       "      <td>Explosion now confirmed at a home in Fort McMu...</td>\n",
       "      <td>Fort McMurray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27103</th>\n",
       "      <td>ID_1111689287857332224</td>\n",
       "      <td>OMAHA! Thank you for coming out last night. Ju...</td>\n",
       "      <td>Nebraska OMAHA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26167</th>\n",
       "      <td>ID_1110802600180568064</td>\n",
       "      <td>While you go about your day, remember those wh...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42278</th>\n",
       "      <td>ID_771458098426966016</td>\n",
       "      <td>@gillhp  rich got woken up by it | tsunami war...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_id  \\\n",
       "17903  ID_1062043965636132864   \n",
       "37119  ID_1176558963938254848   \n",
       "43773   ID_784649154194710528   \n",
       "71527   ID_914176142327914496   \n",
       "14981  ID_1041985289210785792   \n",
       "69724   ID_913251069802692608   \n",
       "40724   ID_732607952192835584   \n",
       "27103  ID_1111689287857332224   \n",
       "26167  ID_1110802600180568064   \n",
       "42278   ID_771458098426966016   \n",
       "\n",
       "                                                    text  \\\n",
       "17903  As three record-breaking wild fires continue t...   \n",
       "37119  4/4 Doctors and paramedics are providing neces...   \n",
       "43773  Flash Flood Warning for Bamberg, Calhoun, Clar...   \n",
       "71527       Now this is something I can trust. DONATE!!!   \n",
       "14981  Death toll so far from #HurricaneFlorence 34Tr...   \n",
       "69724  Hurricane Maria : Charity Navigator list of ch...   \n",
       "40724  Explosion now confirmed at a home in Fort McMu...   \n",
       "27103  OMAHA! Thank you for coming out last night. Ju...   \n",
       "26167  While you go about your day, remember those wh...   \n",
       "42278  @gillhp  rich got woken up by it | tsunami war...   \n",
       "\n",
       "                                                location  \n",
       "17903                                         California  \n",
       "37119                                                NaN  \n",
       "43773  Bamberg Calhoun Clarendon Orangeburg Richland ...  \n",
       "71527                                                NaN  \n",
       "14981                                        Puerto Rico  \n",
       "69724                                                NaN  \n",
       "40724                                      Fort McMurray  \n",
       "27103                                     Nebraska OMAHA  \n",
       "26167                                                NaN  \n",
       "42278                                                NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                               ID_1001136696589631488\n",
       "text        Flash floods struck a Maryland city on Sunday,...\n",
       "location                                             Maryland\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flash floods struck a Maryland city on Sunday, washing out streets and tossing cars like bath toys.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: False\n",
      "Live: False\n",
      "City: True\n",
      ":: False\n",
      "Aerials: False\n",
      "of: False\n",
      "damage: False\n",
      "after: False\n",
      "historic: False\n",
      "flash: False\n",
      "flooding: False\n",
      "in: False\n",
      "Ellicott: True\n",
      "City: True\n",
      ",: False\n",
      "Maryland: True\n",
      ".: False\n"
     ]
    }
   ],
   "source": [
    "# Huge discussion with GPT to finally came out with this code\n",
    "import re\n",
    "\n",
    "def split_and_clean(sentence):\n",
    "\t# Split sentence into words while keeping punctuation as separate tokens\n",
    "\twords = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "\treturn words\n",
    "\n",
    "def normalize_word(word):\n",
    "\t# Remove leading # and @ symbols for comparison purposes\n",
    "\treturn word.lstrip(\"#@\")\n",
    "\n",
    "def annotate_words(sentence_a, sentence_b):\n",
    "\t# Split both sentences into lists of words\n",
    "\twords_a = split_and_clean(sentence_a)\n",
    "\twords_b = split_and_clean(sentence_b)\n",
    "\t\n",
    "\t# Create a set of normalized words and phrases from B\n",
    "\tphrases_b = set()\n",
    "\ti = 0\n",
    "\twhile i < len(words_b):\n",
    "\t\t# Add both individual words and 2-word phrases for matching\n",
    "\t\tphrases_b.add(normalize_word(words_b[i]))\n",
    "\t\tif i < len(words_b) - 1:\n",
    "\t\t\tphrases_b.add(f\"{normalize_word(words_b[i])} {normalize_word(words_b[i+1])}\")\n",
    "\t\ti += 1\n",
    "\n",
    "\tannotated_words = []\n",
    "\ti = 0\n",
    "\n",
    "\twhile i < len(words_a):\n",
    "\t\t# Check if current and next word in A match a 2-word phrase in B\n",
    "\t\tif i < len(words_a) - 1:\n",
    "\t\t\tcurrent_phrase = f\"{normalize_word(words_a[i])} {normalize_word(words_a[i+1])}\"\n",
    "\t\t\tif current_phrase in phrases_b:\n",
    "\t\t\t\tannotated_words.append((words_a[i], True))  # Mark first word of phrase as True\n",
    "\t\t\t\tannotated_words.append((words_a[i+1], True))  # Mark second word of phrase as True\n",
    "\t\t\t\ti += 2  # Skip to next after the phrase\n",
    "\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# Check individual word\n",
    "\t\tannotated_words.append((words_a[i], normalize_word(words_a[i]) in phrases_b))\n",
    "\t\ti += 1\n",
    "\t\n",
    "\treturn annotated_words\n",
    "\n",
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in Ellicott City, Maryland.\"\n",
    "B = \"Ellicott City Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "\tprint(f\"{word}: {is_in_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: False\n",
      "Live: False\n",
      "City: True\n",
      ":: False\n",
      "Aerials: False\n",
      "of: False\n",
      "damage: False\n",
      "after: False\n",
      "historic: False\n",
      "flash: False\n",
      "flooding: False\n",
      "in: False\n",
      "Ellicott: True\n",
      "City: True\n",
      ",: False\n",
      "Maryland: True\n",
      ".: False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_and_clean(sentence):\n",
    "\t# Split sentence into words while keeping punctuation as separate tokens\n",
    "\twords = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "\treturn words\n",
    "\n",
    "def normalize_word(word):\n",
    "\t# Remove leading # and @ symbols for comparison purposes\n",
    "\treturn word.lstrip(\"#@\")\n",
    "\n",
    "def annotate_words(sentence_a, sentence_b):\n",
    "\t# Split both sentences into lists of words\n",
    "\twords_a = split_and_clean(sentence_a)\n",
    "\twords_b = split_and_clean(sentence_b)\n",
    "\t\n",
    "\t# Create a set of normalized words and 2-word phrases from B\n",
    "\tphrases_b = set()\n",
    "\ti = 0\n",
    "\twhile i < len(words_b):\n",
    "\t\t# Add both individual words and 2-word phrases for matching\n",
    "\t\tphrases_b.add(normalize_word(words_b[i]))\n",
    "\t\tif i < len(words_b) - 1:\n",
    "\t\t\tphrases_b.add(f\"{normalize_word(words_b[i])} {normalize_word(words_b[i+1])}\")\n",
    "\t\ti += 1\n",
    "\n",
    "\tannotated_words = []\n",
    "\ti = 0\n",
    "\n",
    "\twhile i < len(words_a):\n",
    "\t\t# Check if current and next word in A match a 2-word phrase in B\n",
    "\t\tif i < len(words_a) - 1:\n",
    "\t\t\tcurrent_phrase = f\"{normalize_word(words_a[i])} {normalize_word(words_a[i+1])}\"\n",
    "\t\t\tif current_phrase in phrases_b:\n",
    "\t\t\t\tannotated_words.append((words_a[i], True))  # Mark first word of phrase as True\n",
    "\t\t\t\tannotated_words.append((words_a[i+1], True))  # Mark second word of phrase as True\n",
    "\t\t\t\ti += 2  # Skip to next after the phrase\n",
    "\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t# Check individual word (only if it's not part of a multi-word match)\n",
    "\t\tannotated_words.append((words_a[i], normalize_word(words_a[i]) in phrases_b))\n",
    "\t\ti += 1\n",
    "\t\n",
    "\treturn annotated_words\n",
    "\n",
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in Ellicott City, Maryland.\"\n",
    "B = \"Ellicott City Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "\tprint(f\"{word}: {is_in_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: False\n",
      "Live: False\n",
      "City: True\n",
      ":: False\n",
      "Aerials: False\n",
      "of: False\n",
      "damage: False\n",
      "after: False\n",
      "historic: False\n",
      "flash: False\n",
      "flooding: False\n",
      "in: False\n",
      "Ellicott: True\n",
      "City: True\n",
      ",: False\n",
      "Maryland: True\n",
      ".: False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_and_clean(sentence):\n",
    "\t# Split sentence into words while keeping punctuation as separate tokens\n",
    "\twords = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "\treturn words\n",
    "\n",
    "def normalize_word(word):\n",
    "\t# Remove leading # and @ symbols for comparison purposes\n",
    "\treturn word.lstrip(\"#@\")\n",
    "\n",
    "def find_multiword_phrases(words_a, words_b):\n",
    "\t# Find all possible multi-word sequences in B that are present in A\n",
    "\tphrases_b = []\n",
    "\tlen_b = len(words_b)\n",
    "\n",
    "\tfor start in range(len_b):\n",
    "\t\tfor end in range(start + 1, len_b + 1):\n",
    "\t\t\tphrase = \" \".join(normalize_word(word) for word in words_b[start:end])\n",
    "\t\t\tif phrase in \" \".join(normalize_word(word) for word in words_a):\n",
    "\t\t\t\tphrases_b.append(phrase)\n",
    "\t\n",
    "\treturn phrases_b\n",
    "\n",
    "def annotate_words(sentence_a, sentence_b):\n",
    "\t# Split both sentences into lists of words\n",
    "\twords_a = split_and_clean(sentence_a)\n",
    "\twords_b = split_and_clean(sentence_b)\n",
    "\t\n",
    "\t# Find all multi-word phrases in B that are in A\n",
    "\tmultiword_phrases_b = find_multiword_phrases(words_a, words_b)\n",
    "\t\n",
    "\t# Annotate words in A\n",
    "\tannotated_words = []\n",
    "\ti = 0\n",
    "\t\n",
    "\twhile i < len(words_a):\n",
    "\t\t# Check if the current word starts a multi-word phrase in B\n",
    "\t\tfound_match = False\n",
    "\t\tfor phrase in multiword_phrases_b:\n",
    "\t\t\tphrase_words = phrase.split()\n",
    "\t\t\tphrase_len = len(phrase_words)\n",
    "\t\t\t\n",
    "\t\t\t# Check if the next `phrase_len` words in A match the phrase\n",
    "\t\t\tif words_a[i:i + phrase_len] == phrase_words:\n",
    "\t\t\t\t# Mark the entire phrase as True\n",
    "\t\t\t\tfor j in range(phrase_len):\n",
    "\t\t\t\t\tannotated_words.append((words_a[i + j], True))\n",
    "\t\t\t\ti += phrase_len  # Skip the words in the phrase\n",
    "\t\t\t\tfound_match = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\tif not found_match:\n",
    "\t\t\t# If no phrase is found, annotate the word normally\n",
    "\t\t\tannotated_words.append((words_a[i], normalize_word(words_a[i]) in words_b))\n",
    "\t\t\ti += 1\n",
    "\t\n",
    "\treturn annotated_words\n",
    "\n",
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in Ellicott City, Maryland.\"\n",
    "B = \"Ellicott City Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "\tprint(f\"{word}: {is_in_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: False\n",
      "Live: False\n",
      "City: True\n",
      ":: False\n",
      "Aerials: False\n",
      "of: False\n",
      "damage: False\n",
      "after: False\n",
      "historic: False\n",
      "flash: False\n",
      "flooding: False\n",
      "in: False\n",
      "Ellicott: True\n",
      "City: True\n",
      ",: False\n",
      "Maryland: True\n",
      ".: False\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def split_and_clean(sentence):\n",
    "\t# Split sentence into words while keeping punctuation as separate tokens\n",
    "\twords = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "\treturn words\n",
    "\n",
    "def normalize_word(word):\n",
    "\t# Remove leading # and @ symbols for comparison purposes\n",
    "\treturn word.lstrip(\"#@\")\n",
    "\n",
    "def find_multiword_phrases(words_a, words_b):\n",
    "\tphrases_b = []\n",
    "\tlen_a = len(words_a)\n",
    "\tlen_b = len(words_b)\n",
    "\t\n",
    "\t# Join words_a and words_b into normalized space-separated strings\n",
    "\tnormalized_a = \" \".join(normalize_word(word) for word in words_a)\n",
    "\t\n",
    "\t# Start from any word in B and create multi-word phrases\n",
    "\tfor start in range(len_b):\n",
    "\t\tfor end in range(start + 1, len_b + 1):\n",
    "\t\t\t# Form a phrase from words_b[start:end]\n",
    "\t\t\tphrase = \" \".join(normalize_word(word) for word in words_b[start:end])\n",
    "\t\t\tif phrase in normalized_a:\n",
    "\t\t\t\tphrases_b.append(phrase)\n",
    "\t\n",
    "\treturn phrases_b\n",
    "\n",
    "\n",
    "def annotate_words(sentence_a, sentence_b):\n",
    "\t# Split both sentences into lists of words\n",
    "\twords_a = split_and_clean(sentence_a)\n",
    "\twords_b = split_and_clean(sentence_b)\n",
    "\t\n",
    "\t# Find all multi-word phrases in B that are in A\n",
    "\tmultiword_phrases_b = find_multiword_phrases(words_a, words_b)\n",
    "\t\n",
    "\t# Annotate words in A\n",
    "\tannotated_words = []\n",
    "\ti = 0\n",
    "\t\n",
    "\twhile i < len(words_a):\n",
    "\t\t# Check if the current word starts a multi-word phrase in B\n",
    "\t\tfound_match = False\n",
    "\t\tfor phrase in multiword_phrases_b:\n",
    "\t\t\tphrase_words = phrase.split()\n",
    "\t\t\tphrase_len = len(phrase_words)\n",
    "\t\t\t\n",
    "\t\t\t# Check if the next `phrase_len` words in A match the phrase\n",
    "\t\t\tif words_a[i:i + phrase_len] == phrase_words:\n",
    "\t\t\t\t# Mark the entire phrase as True\n",
    "\t\t\t\tfor j in range(phrase_len):\n",
    "\t\t\t\t\tannotated_words.append((words_a[i + j], True))\n",
    "\t\t\t\ti += phrase_len  # Skip the words in the phrase\n",
    "\t\t\t\tfound_match = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\n",
    "\t\tif not found_match:\n",
    "\t\t\t# If no phrase is found, annotate the word normally\n",
    "\t\t\tannotated_words.append((words_a[i], normalize_word(words_a[i]) in words_b))\n",
    "\t\t\ti += 1\n",
    "\t\n",
    "\treturn annotated_words\n",
    "\n",
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in Ellicott City, Maryland.\"\n",
    "B = \"Ellicott City Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "\tprint(f\"{word}: {is_in_b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ellicott City', 'Love', 'Maryland']\n"
     ]
    }
   ],
   "source": [
    "def find_multiword_phrases(words_a, words_b):\n",
    "\tphrases_b = []\n",
    "\tlen_b = len(words_b)\n",
    "\t\n",
    "\t# Join words_a into a normalized space-separated string\n",
    "\tnormalized_a = \" \".join(normalize_word(word) for word in words_a)\n",
    "\t\n",
    "\ti = 0\n",
    "\twhile i < len_b:\n",
    "\t\t# Try to form the longest possible phrase from words_b that exists in words_a\n",
    "\t\tfor end in range(len_b, i, -1):  # Start checking from the longest possible phrase\n",
    "\t\t\tphrase = \" \".join(normalize_word(word) for word in words_b[i:end])\n",
    "\t\t\tif phrase in normalized_a:\n",
    "\t\t\t\tphrases_b.append(phrase)  # Add the phrase\n",
    "\t\t\t\ti = end - 1  # Skip the words in the phrase\n",
    "\t\t\t\tbreak\n",
    "\t\ti += 1\n",
    "\t\n",
    "\treturn phrases_b\n",
    "\n",
    "# Example input sentences\n",
    "words_a = split_and_clean(\"Watch Live City: Aerials of damage after historic flash flooding Love in Ellicott City, Maryland.\")\n",
    "words_b = split_and_clean(\"Ellicott City Love Maryland\")\n",
    "\n",
    "# Call find_multiword_phrases\n",
    "phrases = find_multiword_phrases(words_a, words_b)\n",
    "\n",
    "# Output the found phrases\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: False\n",
      "Live: False\n",
      "City: True\n",
      ":: False\n",
      "Aerials: False\n",
      "of: False\n",
      "damage: False\n",
      "after: False\n",
      "historic: False\n",
      "flash: False\n",
      "flooding: False\n",
      "in: False\n",
      "Ellicott: True\n",
      "City: True\n",
      ",: False\n",
      "Maryland: True\n",
      ".: False\n"
     ]
    }
   ],
   "source": [
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in Ellicott City, Maryland.\"\n",
    "B = \"Ellicott City Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "\tprint(f\"{word}: {is_in_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: O\n",
      "Live: O\n",
      "City: O\n",
      ":: O\n",
      "Aerials: O\n",
      "of: O\n",
      "damage: O\n",
      "after: O\n",
      "historic: O\n",
      "flash: O\n",
      "flooding: O\n",
      "in: O\n",
      "City: B-LOC\n",
      "Ellicott: I-LOC\n",
      ",: O\n",
      "Maryland: B-LOC\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def split_and_clean(sentence):\n",
    "    # Split sentence into words while keeping punctuation as separate tokens\n",
    "    words = re.findall(r\"[\\w]+|[^\\s\\w]\", sentence)\n",
    "    return words\n",
    "\n",
    "\n",
    "def normalize_word(word):\n",
    "    # Remove leading # and @ symbols for comparison purposes\n",
    "    return word.lstrip(\"#@\")\n",
    "\n",
    "\n",
    "def find_multiword_phrases(words_a, words_b) -> list[str]:\n",
    "    phrases_b = []\n",
    "    len_b = len(words_b)\n",
    "\n",
    "    # Join words_a into a normalized space-separated string\n",
    "    normalized_a = \" \".join(normalize_word(word) for word in words_a)\n",
    "\n",
    "    i = 0\n",
    "    while i < len_b:\n",
    "        # Try to form the longest possible phrase from words_b that exists in words_a\n",
    "        for end in range(\n",
    "            len_b, i, -1\n",
    "        ):  # Start checking from the longest possible phrase\n",
    "            phrase = \" \".join(normalize_word(word) for word in words_b[i:end])\n",
    "            if phrase in normalized_a:\n",
    "                phrases_b.append(phrase)  # Add the phrase\n",
    "                i = end - 1  # Skip the words in the phrase\n",
    "                break\n",
    "        i += 1\n",
    "\n",
    "    return phrases_b\n",
    "\n",
    "\n",
    "def annotate_words(sentence_a, sentence_b):\n",
    "    # Split both sentences into lists of words\n",
    "    words_a = split_and_clean(sentence_a)\n",
    "    words_b = split_and_clean(sentence_b)\n",
    "\n",
    "    # Find all multi-word phrases in B that are in A\n",
    "    words_b = find_multiword_phrases(words_a, words_b)\n",
    "\n",
    "    # Annotate words in A\n",
    "    annotated_words = []\n",
    "    n_ignored = set()\n",
    "    for i, word in enumerate(words_a):\n",
    "        if i in n_ignored:\n",
    "            continue\n",
    "        possible_match = [w for w in words_b if w.startswith(word)]\n",
    "        if not possible_match:\n",
    "            annotated_words.append((word, \"O\"))\n",
    "            continue\n",
    "        decision = [\n",
    "            \" \".join(words_a[i : i + len(w.split())]) == w for w in possible_match\n",
    "        ]\n",
    "        if any(decision):\n",
    "            dec, w = sorted(\n",
    "                zip(decision, possible_match), key=lambda x: (-x[0], -len(x[1].split()))\n",
    "            )[0]\n",
    "            n_ignored.update([n for n in range(i, i + len(w.split()))])\n",
    "\n",
    "            annotated_words.extend(\n",
    "                [(w, \"B-LOC\" if k == 0 else \"I-LOC\") for k, w in enumerate(words_a[i : i + len(w.split())])]\n",
    "            )\n",
    "        else: # Fake starting match !\n",
    "            annotated_words.append((word, \"O\"))\n",
    "\n",
    "    return annotated_words\n",
    "\n",
    "\n",
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in City Ellicott, Maryland.\"\n",
    "B = \"City Ellicott Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "    print(f\"{word}: {is_in_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch: O\n",
      "Live: O\n",
      "City: O\n",
      ":: O\n",
      "Aerials: O\n",
      "of: O\n",
      "damage: O\n",
      "after: O\n",
      "historic: O\n",
      "flash: O\n",
      "flooding: O\n",
      "in: O\n",
      "City: O\n",
      "Ellicott: O\n",
      ",: O\n",
      "Maryland: O\n",
      ".: O\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in City Ellicott, Maryland.\"\n",
    "B = \"\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotations = annotate_words(A, B)\n",
    "\n",
    "# Print annotated words\n",
    "for word, is_in_b in annotations:\n",
    "    print(f\"{word}: {is_in_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id       0\n",
       "text           0\n",
       "location    4599\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36637</th>\n",
       "      <td>ID_1176505873419689984</td>\n",
       "      <td>No damages have been reported so far in #Kashm...</td>\n",
       "      <td>Kashmir Valley Mirpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34825</th>\n",
       "      <td>ID_1168308758998671360</td>\n",
       "      <td>JUST IN: I-26 lane reversal starting at NOON o...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39355</th>\n",
       "      <td>ID_728667676155879424</td>\n",
       "      <td>Our &lt;3 goes out to those affected by Fort McMu...</td>\n",
       "      <td>McMurray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36075</th>\n",
       "      <td>ID_1176473682216202240</td>\n",
       "      <td>Yah ALLAH reham ! Karachi damaged by the 10 mi...</td>\n",
       "      <td>Pakistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41539</th>\n",
       "      <td>ID_768563495629717504</td>\n",
       "      <td>• #ItalyEarthquake • The highest number of vic...</td>\n",
       "      <td>Amatrice Italy Rieti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48462</th>\n",
       "      <td>ID_902637254660812800</td>\n",
       "      <td>#Rockport and #AransasCounty need your help! T...</td>\n",
       "      <td>AransasCounty Houston</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20646</th>\n",
       "      <td>ID_1064659382607110144</td>\n",
       "      <td>Most local governments dealing with California...</td>\n",
       "      <td>California</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45723</th>\n",
       "      <td>ID_870152565601828864</td>\n",
       "      <td>RT @AmanthaP: APPEAL @SLRedCross running low o...</td>\n",
       "      <td>Galle Gampaha Matara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8974</th>\n",
       "      <td>ID_1035336737868767232</td>\n",
       "      <td>RT @Anulalindia: Water rose to the level of tw...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>ID_1022095810014703616</td>\n",
       "      <td>People at Thriasio hospital waiting to donate ...</td>\n",
       "      <td>Thriasio hospital</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tweet_id  \\\n",
       "36637  ID_1176505873419689984   \n",
       "34825  ID_1168308758998671360   \n",
       "39355   ID_728667676155879424   \n",
       "36075  ID_1176473682216202240   \n",
       "41539   ID_768563495629717504   \n",
       "48462   ID_902637254660812800   \n",
       "20646  ID_1064659382607110144   \n",
       "45723   ID_870152565601828864   \n",
       "8974   ID_1035336737868767232   \n",
       "1263   ID_1022095810014703616   \n",
       "\n",
       "                                                    text  \\\n",
       "36637  No damages have been reported so far in #Kashm...   \n",
       "34825  JUST IN: I-26 lane reversal starting at NOON o...   \n",
       "39355  Our <3 goes out to those affected by Fort McMu...   \n",
       "36075  Yah ALLAH reham ! Karachi damaged by the 10 mi...   \n",
       "41539  • #ItalyEarthquake • The highest number of vic...   \n",
       "48462  #Rockport and #AransasCounty need your help! T...   \n",
       "20646  Most local governments dealing with California...   \n",
       "45723  RT @AmanthaP: APPEAL @SLRedCross running low o...   \n",
       "8974   RT @Anulalindia: Water rose to the level of tw...   \n",
       "1263   People at Thriasio hospital waiting to donate ...   \n",
       "\n",
       "                    location  \n",
       "36637  Kashmir Valley Mirpur  \n",
       "34825                         \n",
       "39355               McMurray  \n",
       "36075               Pakistan  \n",
       "41539   Amatrice Italy Rieti  \n",
       "48462  AransasCounty Houston  \n",
       "20646             California  \n",
       "45723   Galle Gampaha Matara  \n",
       "8974                          \n",
       "1263       Thriasio hospital  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.fillna(\"\", inplace=True)\n",
    "\n",
    "train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'• #ItalyEarthquake • The highest number of victims in #Rieti. #Amatrice is actually a ghost town. Death toll: 159. #earthquake #Italy'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[41539, \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16448/16448 [00:00<00:00, 87838.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "thr = ThreadPoolExecutor()\n",
    "results = list(\n",
    "    tqdm(\n",
    "        thr.map(annotate_words, train[\"text\"].values, train[\"location\"].values),\n",
    "        total=len(train),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Flash', 'O'),\n",
       " ('floods', 'O'),\n",
       " ('struck', 'O'),\n",
       " ('a', 'O'),\n",
       " ('Maryland', 'B-LOC'),\n",
       " ('city', 'O'),\n",
       " ('on', 'O'),\n",
       " ('Sunday', 'O'),\n",
       " (',', 'O'),\n",
       " ('washing', 'O'),\n",
       " ('out', 'O'),\n",
       " ('streets', 'O'),\n",
       " ('and', 'O'),\n",
       " ('tossing', 'O'),\n",
       " ('cars', 'O'),\n",
       " ('like', 'O'),\n",
       " ('bath', 'O'),\n",
       " ('toys', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                               ID_1001136696589631488\n",
       "text        Flash floods struck a Maryland city on Sunday,...\n",
       "location                                             Maryland\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_words(sentence_a, sentence_b):\n",
    "    # Split both sentences into lists of words\n",
    "    words_a = split_and_clean(sentence_a)\n",
    "    words_b = split_and_clean(sentence_b)\n",
    "\n",
    "    # Find all multi-word phrases in B that are in A\n",
    "    words_b = find_multiword_phrases(words_a, words_b)\n",
    "\n",
    "    # Annotate words in A\n",
    "    annotated_words = []\n",
    "    n_ignored = set()\n",
    "    entities = []\n",
    "    for i, word in enumerate(words_a):\n",
    "        if i in n_ignored:\n",
    "            continue\n",
    "        possible_match = [w for w in words_b if w.startswith(word)]\n",
    "        if not possible_match:\n",
    "            annotated_words.append((word, \"O\"))\n",
    "            continue\n",
    "        decision = [\n",
    "            \" \".join(words_a[i : i + len(w.split())]) == w for w in possible_match\n",
    "        ]\n",
    "        if any(decision):\n",
    "            dec, w = sorted(\n",
    "                zip(decision, possible_match), key=lambda x: (-x[0], -len(x[1].split()))\n",
    "            )[0]\n",
    "            n_ignored.update([n for n in range(i, i + len(w.split()))])\n",
    "\n",
    "            entities.append([i, i - 1 + len(w.split())])\n",
    "\n",
    "            annotated_words.extend(\n",
    "                [\n",
    "                    (w, \"B-LOC\" if k == 0 else \"I-LOC\")\n",
    "                    for k, w in enumerate(words_a[i : i + len(w.split())])\n",
    "                ]\n",
    "            )\n",
    "        else:  # Fake starting match !\n",
    "            annotated_words.append((word, \"O\"))\n",
    "\n",
    "    return annotated_words, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Watch', 'O'),\n",
       "  ('Live', 'O'),\n",
       "  ('City', 'O'),\n",
       "  (':', 'O'),\n",
       "  ('Aerials', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('damage', 'O'),\n",
       "  ('after', 'O'),\n",
       "  ('historic', 'O'),\n",
       "  ('flash', 'O'),\n",
       "  ('flooding', 'O'),\n",
       "  ('in', 'O'),\n",
       "  ('City', 'B-LOC'),\n",
       "  ('Ellicott', 'I-LOC'),\n",
       "  (',', 'O'),\n",
       "  ('Maryland', 'B-LOC'),\n",
       "  ('.', 'O')],\n",
       " [[12, 13], [15, 15]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example sentences\n",
    "A = \"Watch Live City: Aerials of damage after historic flash flooding in City Ellicott, Maryland.\"\n",
    "B = \"City Ellicott Maryland\"\n",
    "\n",
    "# Annotate words in A\n",
    "annotate_words(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16448/16448 [00:00<00:00, 75682.56it/s]\n"
     ]
    }
   ],
   "source": [
    "results_with_annotation = list(\n",
    "    tqdm(\n",
    "        thr.map(annotate_words, train[\"text\"].values, train[\"location\"].values),\n",
    "        total=len(train),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Howard', 'B-LOC'),\n",
       "  ('County', 'O'),\n",
       "  ('Executive', 'O'),\n",
       "  ('Allan', 'O'),\n",
       "  ('Kittleman', 'O'),\n",
       "  ('said', 'O'),\n",
       "  ('Monday', 'O'),\n",
       "  ('morning', 'O'),\n",
       "  ('that', 'O'),\n",
       "  ('his', 'O'),\n",
       "  ('priorities', 'O'),\n",
       "  ('are', 'O'),\n",
       "  ('finding', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('missing', 'O'),\n",
       "  ('man', 'O'),\n",
       "  ('and', 'O'),\n",
       "  ('assessing', 'O'),\n",
       "  ('the', 'O'),\n",
       "  ('condition', 'O'),\n",
       "  ('of', 'O'),\n",
       "  ('buildings', 'O'),\n",
       "  ('that', 'O'),\n",
       "  ('house', 'O'),\n",
       "  ('shops', 'O'),\n",
       "  (',', 'O'),\n",
       "  ('restaurants', 'O'),\n",
       "  ('and', 'O'),\n",
       "  ('families', 'O'),\n",
       "  ('.', 'O')],\n",
       " [[0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_with_annotation[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3329539"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [\n",
    "    \"\\n\".join(f\"{w}\\t{lab}\" for w, lab in preds) for preds, _ in results_with_annotation\n",
    "]\n",
    "\n",
    "text_train = \"\\n\\n\".join(lines)\n",
    "len(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/accepted_data/FullTrainCleaned.txt\", \"w\") as file:\n",
    "    file.write(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_text': ['Flash',\n",
       "  'floods',\n",
       "  'struck',\n",
       "  'a',\n",
       "  'Maryland',\n",
       "  'city',\n",
       "  'on',\n",
       "  'Sunday',\n",
       "  ',',\n",
       "  'washing',\n",
       "  'out',\n",
       "  'streets',\n",
       "  'and',\n",
       "  'tossing',\n",
       "  'cars',\n",
       "  'like',\n",
       "  'bath',\n",
       "  'toys',\n",
       "  '.'],\n",
       " 'ner': [[4, 4, 'location']]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_annotated = [\n",
    "    {\n",
    "        \"tokenized_text\": [w for w, _ in preds],\n",
    "        \"ner\": [tag + [\"location\"] for tag in tags],\n",
    "    } for preds, tags in results_with_annotation\n",
    "]\n",
    "\n",
    "lines_annotated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/accepted_data/FullTrainCleaned.json\", \"w\") as file:\n",
    "    json.dump(lines_annotated, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.068033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.015442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             NLabel\n",
       "count  16448.000000\n",
       "mean       1.068033\n",
       "std        1.015442\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        1.000000\n",
       "75%        1.000000\n",
       "max       17.000000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_count = pd.DataFrame([\n",
    "    len(i[\"ner\"]) for i in lines_annotated\n",
    "], columns=[\"NLabel\"])\n",
    "\n",
    "location_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLabel\n",
       "1         8058\n",
       "0         4608\n",
       "2         2570\n",
       "3          797\n",
       "4          254\n",
       "5           82\n",
       "6           42\n",
       "7           22\n",
       "8            6\n",
       "9            4\n",
       "12           2\n",
       "10           1\n",
       "11           1\n",
       "17           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLabel\n",
       "1         8058\n",
       "0         4608\n",
       "2         2570\n",
       "3          797\n",
       "4          254\n",
       "5           82\n",
       "6           79\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_count.loc[location_count[\"NLabel\"] > 5, \"NLabel\"] = 6\n",
    "\n",
    "location_count.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14803, 1645)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_index, test_index = train_test_split(\n",
    "    location_count, test_size=.1, stratify=location_count[\"NLabel\"].values\n",
    ")\n",
    "\n",
    "len(train_index), len(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLabel\n",
       "1    7252\n",
       "0    4147\n",
       "2    2313\n",
       "3     717\n",
       "4     229\n",
       "5      74\n",
       "6      71\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index[\"NLabel\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLabel\n",
       "1    806\n",
       "0    461\n",
       "2    257\n",
       "3     80\n",
       "4     25\n",
       "6      8\n",
       "5      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index[\"NLabel\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines_annotated = [\n",
    "    lines_annotated[i] for i in train_index.index\n",
    "]\n",
    "\n",
    "test_lines_annotated = [\n",
    "    lines_annotated[i] for i in test_index.index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/accepted_data\", exist_ok=True)\n",
    "\n",
    "with open(\"data/accepted_data/TrainCleaned.json\", \"w\") as file:\n",
    "    json.dump(train_lines_annotated, file, indent=4)\n",
    "\n",
    "with open(\"data/accepted_data/TestCleaned.json\", \"w\") as file:\n",
    "    json.dump(test_lines_annotated, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [\n",
    "    lines[i] for i in train_index.index\n",
    "]\n",
    "\n",
    "test_lines = [\n",
    "    lines[i] for i in test_index.index\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"data/accepted_data\", exist_ok=True)\n",
    "\n",
    "with open(\"data/accepted_data/TrainCleaned.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(train_lines), )\n",
    "\n",
    "with open(\"data/accepted_data/TestCleaned.txt\", \"w\") as file:\n",
    "    file.write(\"\\n\\n\".join(test_lines), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
